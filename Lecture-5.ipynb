{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 5.\n",
    "# Regression problem: quality measures and regularization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reminder\n",
    "\n",
    "- Regression problem statement\n",
    "- Linear least squares\n",
    "- Nonlinear least squares\n",
    "- Gauss-Newton and Levenberg-Marquardt methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Today lecture\n",
    "\n",
    "- Regression quality measures\n",
    "- What to do when the design matrix is not full-rank?\n",
    "- Probabilistic interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quality measures\n",
    "\n",
    "- Train and test loss\n",
    "- Coefficient of determination $R^2$\n",
    "$$\n",
    "R^2 = 1 - \\frac{S_{res}}{S_{tot}},\n",
    "$$\n",
    "where $S_{res} = \\| y - \\hat{y} \\|^2_2$ and $S_{tot} = \\| y - \\bar{y}\\|^2_2$.\n",
    "Closer to 1, better the model\n",
    "\n",
    "Cons: $R^2$ increases when more features are added\n",
    "\n",
    "- Scikit-learn uses this measure to score the regression model \n",
    "- Attempt to treat the cons: adjusted $R^2$ &mdash; $\\bar{R}^2$\n",
    "$$\n",
    "\\bar{R}^2 = 1 - (1 - R^2) \\frac{m-1}{m-n-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AIC and BIC - the lower the better\n",
    "\n",
    "- Akaike Information Criterion\n",
    "$$\n",
    "AIC = m \\log \\left(\\frac{S_{res}}{m}\\right) + 2n,\n",
    "$$\n",
    "where $m$ is a number of samples, $n$ is a number of features\n",
    "- Bayesian Information Criterion\n",
    "$$\n",
    "BIC = m \\log \\left(\\frac{S_{res}}{m}\\right) + 2n \\log(m)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Statistical tests\n",
    "\n",
    "- $t$-test\n",
    "\n",
    "Test significance of every feature in explanation of the target vector:\n",
    "\n",
    "$$\n",
    "H_0: w_j = 0 \\quad H_1: w_j \\neq 0\n",
    "$$\n",
    "\n",
    "- $F$-test\n",
    "\n",
    "Test if the linear model is adequate\n",
    "\n",
    "$$\n",
    "H_0: w_1, w_2, \\ldots, w_n = 0 \\quad H_1: \\exists k: \\; w_k \\neq 0 \n",
    "$$\n",
    "\n",
    "- Normality of the residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multicollinearity\n",
    "\n",
    "**Definition.** The phenomenon of linear dependencies between features is called *multicollinearity*.\n",
    "\n",
    "**Corollary.** The matrix $X^{\\top}X$ is singular and normal equation can not be solved as it is. \n",
    "\n",
    "**Q:** how to deal with this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization\n",
    "\n",
    "**Definition. (not formal)** Regularization is a process of introduction to the model additional information to solve incorrect problems\n",
    "\n",
    "Examples:\n",
    "- improve stability by changing objective function\n",
    "- provide uniqueness of the solution by adding constraints\n",
    "- transform objective function to provide finite solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tikhonov regularization (Ridge or $\\ell_2$)\n",
    "\n",
    "$$\n",
    "\\min_w \\frac{1}{2}\\|Xw - y \\|^2_2 + \\frac{\\alpha}{2}\\|w\\|^2_2, \\quad \\alpha > 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Lasso ($\\ell_1$)\n",
    "\n",
    "$$\n",
    "\\min_w \\|Xw - y \\|^2_2 + \\alpha\\|w\\|_1, \\quad \\alpha > 0\n",
    "$$\n",
    "\n",
    "Specifics:\n",
    "- nondifferential but convex objective function\n",
    "- relaxation of the $\\ell_0$ \"norm\"\n",
    "- sparse solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ridge vs. Lasso\n",
    "<img src=\"./fig/l1_l2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Elastic Net\n",
    "\n",
    "$$\n",
    "\\min_w \\|Xw - y \\|^2_2 + \\alpha \\rho\\|w\\|_1 + \\alpha\\frac{1-\\rho}{2}\\| w \\|^2_2, \\quad \\rho \\in [0, 1], \\alpha > 0\n",
    "$$\n",
    "\n",
    "Specifics:\n",
    "- combination of Lasso and Ridge\n",
    "- more stable than Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### General form of objectives in machine learning\n",
    "$$\n",
    "\\min_w \\ell(w|X, y) + R(w),\n",
    "$$\n",
    "where $\\ell(w|X, y) = \\sum\\limits_{i=1}^m \\ell_i(w|x_i, y_i)$ is a loss function and $R(w)$ is a regulaization term\n",
    "\n",
    "- The objectives of such structure is called *composite* functions.\n",
    "- For this structure there exist special optimization approaches and theory\n",
    "- Since loss is a sum of loss for every sample, *distributed optimization* is active too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probability interpretation\n",
    "\n",
    "- Maximum likelihood estimation\n",
    "- Normal distributed residuals - linear least squares problem\n",
    "- Laplace distributed residuals - MAE minimization\n",
    "- Bayesian methodology - later in the course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap\n",
    "\n",
    "- Quality mesures for regression problem\n",
    "- Multicollinearity and regularization\n",
    "- Probabilistic interpretation"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
